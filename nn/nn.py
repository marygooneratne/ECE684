'''
Q1
Build a multi-layer neural network using PyTorch. Use it to solve the XOR
classification problem generated by gen_xor.py. Visualize the decision
surface using matplotlib, with the training data overlaid as in Tensorflow
Playground.
'''
import gen_xor
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

torch.manual_seed(2)


class XOR(nn.Module):
    def __init__(self, input_dim=2, output_dim=1):
        super(XOR, self).__init__()
        self.lin1 = nn.Linear(input_dim, 3)
        self.lin2 = nn.Linear(3, output_dim)
        self.weights_init()

    def forward(self, x):
        x = self.lin1(x)
        x = torch.tanh(x)
        x = self.lin2(x)
        x = torch.tanh(x)
        return x

    def weights_init(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # initialize the weight tensor, here we use a normal distribution
                m.weight.data.normal_(0, 1)

    def train(self, X, Y, epochs=2000):

        X = torch.Tensor(X)
        Y = torch.Tensor(Y)

        loss_func = nn.MSELoss()
        optimizer = optim.SGD(self.parameters(), lr=0.02)

        steps = X.size(0)
        for i in range(epochs):
            for j in range(steps):
                data_point = np.random.randint(X.size(0))
                x_var = Variable(X[data_point], requires_grad=False)
                y_var = Variable(Y[data_point], requires_grad=False)
                optimizer.zero_grad()
                y_hat = self(x_var)
                loss = loss_func.forward(y_hat, y_var)
                loss.backward()
                optimizer.step()

            if i % 50 == 0:
                print("Epoch: {0}, Loss: {1}, ".format(
                    i, loss.data.numpy()))
        model_params = list(self.parameters())
        model_weights = model_params[0].data.numpy()
        model_bias = model_params[1].data.numpy()


def plot_boundary_in_original_space(X, Y, model):
    X = torch.Tensor(X)
    Y = torch.Tensor(Y)
    grid_density = 100
    x1 = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, grid_density)
    x2 = np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, grid_density)
    mash = np.meshgrid(x1, x2)

    data_tmp = np.ndarray((grid_density**2, 2))
    data_tmp[:, 0] = mash[0].flatten()
    data_tmp[:, 1] = mash[1].flatten()
    data_tmp_tensor = torch.tensor(data_tmp, dtype=torch.float32)

    preds = model(data_tmp_tensor).detach().numpy()
    print(preds.flatten())
    c0 = data_tmp[preds.flatten() < 0.5]
    c1 = data_tmp[preds.flatten() >= 0.5]
    plt.scatter(c0[:, 0], c0[:, 1], alpha=1.0, marker='s', color="#aaccee")
    plt.scatter(c1[:, 0], c1[:, 1], alpha=1.0, marker='s', color="#eeccaa")
    for y in Y.unique():
        if y == 0:
            plt.scatter(X[Y == y, 0], X[Y == y, 1], s=50)
        else:
            plt.scatter(X[Y == y, 0], X[Y == y, 1], c='red', s=50)
    plt.title('Data and boundary in original space')
    plt.show()


'''
Q2
Implement a feed-forward neural network function from scratch. Extract the
learned weights from Q1 and run the model through your custom implementation.
Demonstrate that you get the same results.
Do not train the model yourself. Do not implement backpropagation. Just
run it forward using the PyTorch-trained weights.
'''


class neural_network(object):
    def __init__(self, input_size=2, output_size=1, hidden_size=3, W1=None, W2=None):
        # parameters
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size

        # weights
        if not W1.any() or not W2.any():
            print("NONE")
            # (3x2) weight matrix from input to hidden layer
            self.W1 = np.random.randn(self.input_size, self.hidden_size)
            # (3x1) weight matrix from hidden to output layer
            self.W2 = np.random.randn(self.hidden_size, self.output_size)
        else:
            self.W1 = W1
            self.W2 = W2

    def forward(self, X):
        # forward propagation through our network
        # dot product of X (input) and first set of 3x2 weights
        self.z = np.dot(X, self.W1)

        self.z2 = self.tanh(self.z)  # activation function

        # dot product of hidden layer (z2) and second set of 3x1 weights
        self.z3 = np.dot(self.z2, self.W2)

        o = self.tanh(self.z3)  # final activation function
        return o

    def tanh(self, s):
        # activation function
        return (np.exp(s)-np.exp(-s))/(np.exp(s)+np.exp(-s))
