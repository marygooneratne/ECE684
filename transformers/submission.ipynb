{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit72de592ab5f2408580af9d1f7a18c19c",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from hw import load_data, Encoder, Decoder, Seq2Seq\n",
    "from transformer import Transformer\n",
    "import string\n",
    "import random\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processed\n"
     ]
    }
   ],
   "source": [
    "# Data and Constants\n",
    "X_train, X_test, X_val, Y_train, Y_val, Y_test = load_data(100)\n",
    "\n",
    "INPUT_DIM = len(X_train[0][0])\n",
    "OUTPUT_DIM = len(Y_train[0][0])\n",
    "ENC_EMB_DIM = 16\n",
    "DEC_EMB_DIM = 16\n",
    "HID_DIM = 32\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "NUM_LAYERS = 2\n",
    "PAD_IDX = 0\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH = 32\n",
    "EMB_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.10\n",
    "FORWARD_EXPANSION = 4\n",
    "MAX_LEN = 40\n",
    "criterion_lstm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "criterion_transformer = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Stuff\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM,\n",
    "                  NUM_LAYERS, ENC_DROPOUT)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,\n",
    "                  NUM_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "lstm = Seq2Seq(enc, dec).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer\n",
    "transformer = Transformer(\n",
    "    EMB_DIM,\n",
    "    INPUT_DIM,\n",
    "    INPUT_DIM,\n",
    "    PAD_IDX,\n",
    "    NUM_HEADS,\n",
    "    NUM_LAYERS,\n",
    "    NUM_LAYERS,\n",
    "    FORWARD_EXPANSION,\n",
    "    DROPOUT,\n",
    "    MAX_LEN,\n",
    "    device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Training\n",
    "def train(model: nn.Module,\n",
    "          X: list,\n",
    "          Y: list,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(X):\n",
    "\n",
    "        src = X[i]\n",
    "        trg = Y[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        \n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Evaluate\n",
    "def evaluate(model: nn.Module,\n",
    "             X: list,\n",
    "             Y: list,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(X):\n",
    "\n",
    "            src = X[i]\n",
    "            trg = Y[i]\n",
    "\n",
    "            output = model(src, trg, 0)  \n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg.view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(X)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "optimizer_lstm = optim.Adam(lstm.parameters(),lr=LEARNING_RATE)\n",
    "optimizer_transformer = optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_transformer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 01 | Time: 0m 5s\n",
      "\tTrain Loss: 3.228 | Train PPL:  25.226\n",
      "\t Val. Loss: 2.902 |  Val. PPL:  18.207\n",
      "Epoch: 02 | Time: 0m 6s\n",
      "\tTrain Loss: 1.268 | Train PPL:   3.553\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.692\n",
      "Epoch: 03 | Time: 0m 4s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 04 | Time: 0m 4s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 05 | Time: 0m 5s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 06 | Time: 0m 4s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 07 | Time: 0m 4s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 08 | Time: 0m 4s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 09 | Time: 0m 4s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 10 | Time: 0m 4s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "| Test Loss: 0.227 | Test PPL:   1.255 |\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(lstm, X_train, Y_train, optimizer_lstm, criterion_lstm, CLIP)\n",
    "        valid_loss = evaluate(lstm, X_val, Y_val, criterion_lstm)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(lstm, X_test, Y_test, criterion_lstm)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 1 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 2 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 3 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 4 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 5 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 6 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 7 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[Epoch 8 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Epoch    11: reducing learning rate of group 0 to 3.0000e-05.\n",
      "[Epoch 9 / 10]\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "# Train Transformer\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"[Epoch {epoch} / {N_EPOCHS}]\")\n",
    "\n",
    "    transformer.eval()\n",
    "\n",
    "    transformer.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(X_train):\n",
    "        \n",
    "        inp_data = batch\n",
    "        target = Y_train[batch_idx]\n",
    "\n",
    "        output = transformer(inp_data, target)\n",
    "\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        \n",
    "        target = target.view(-1) \n",
    "\n",
    "        loss = criterion_transformer(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer_transformer.step()\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    scheduler.step(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator\n",
    "def translate(model, english, pig_latin, device, max_length=50):\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    alphadict = {}\n",
    "    numdict = {}\n",
    "    for i, a in enumerate(alphabet):\n",
    "        alphadict.update({a: i+1})\n",
    "        numdict.update({i+1: a})\n",
    "    english = torch.as_tensor(to_vector(english, alphadict,len(english)))\n",
    "    pig_latin = to_vector(pig_latin, alphadict, len(pig_latin))\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(english.unsqueeze(1).to(device))\n",
    "    outputs = [0]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "        best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == 0:\n",
    "            break\n",
    "    print(outputs)\n",
    "    translated_sentence = [to_letter(oh, numdict) for oh in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(word, alphadict, max_length):\n",
    "    vec = []\n",
    "    for letter in word:\n",
    "        idx = alphadict[letter]\n",
    "        vec.append(one_hot(idx))\n",
    "    while len(vec) < max_length:\n",
    "        vec.append(one_hot(0))\n",
    "    return vec\n",
    "\n",
    "def one_hot(index):\n",
    "    vec = 27 * [0]\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "def to_letter(oh, numdict):\n",
    "    for i, on in enumerate(oh):\n",
    "        if on == 1:\n",
    "            return numdict[i]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-35efd11c88df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apple'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'appleway'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-dbfc6aad9d67>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(model, english, pig_latin, device, max_length)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtranslated_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_letter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# remove start token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-dbfc6aad9d67>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtranslated_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_letter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# remove start token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-64d5b66e644a>\u001b[0m in \u001b[0;36mto_letter\u001b[1;34m(oh, numdict)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mto_letter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnumdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# \n",
    "translate(lstm, 'apple', 'appleway', device, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}