{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# PREPROCESSING\n",
    "def gen_files(length=10000, input_file=\"words.txt\", output_file=\"translated.txt\"):\n",
    "    count = 0\n",
    "    suffixes = [\"ay\", \"way\"]\n",
    "    vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "    with open(input_file, 'r') as input_file:\n",
    "        with open(output_file, 'w') as output_file:\n",
    "            for line in input_file:\n",
    "                # if count > length:\n",
    "                #     break\n",
    "                for word in line.split():\n",
    "                    word = word.lower()\n",
    "                    consanants = \"\"\n",
    "                    translated = \"\"\n",
    "                    while word[0] not in vowels:\n",
    "                        consanants = consanants + word[0]\n",
    "                        if len(word) > 1:\n",
    "                            word = word[1:]\n",
    "                        else: \n",
    "                            word = \"\"\n",
    "                            break\n",
    "                    if len(consanants) > 0:\n",
    "                        translated = word + consanants + suffixes[0]\n",
    "                    else:\n",
    "                        translated = word +suffixes[1]\n",
    "                    output_file.write(translated+\"\\n\")\n",
    "                    count += 1\n",
    "    input_file.close()\n",
    "    output_file.close()\n",
    "\n",
    "def gen_examples(input_file=\"words.txt\", output_file=\"translated.txt\"):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    for line in open(input_file, \"r\"): input_data.append(\"\".join([c for c in line[:-1]]))\n",
    "    for line in open(output_file, \"r\"): output_data.append(\"\".join([c for c in line[:-1]]))\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING\n",
    "def encode_single(data, chars, int_to_char):\n",
    "    char_to_int = {ch: i for i, ch in int_to_char.items()}\n",
    "    en_data = np.array([char_to_int[ch] for ch in data])\n",
    "    return en_data\n",
    "\n",
    "def encode_list(data, chars, int_to_char):\n",
    "    return [encode_single(single, chars, int_to_char) for single in list(data)]\n",
    "\n",
    "def transform(data, vector_size=33):\n",
    "    transformed_data = np.zeros(shape=(vector_size))\n",
    "    for i in range(0, len(data)):\n",
    "        transformed_data[i] = data[i]\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-6d8ff01ecf55>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-6d8ff01ecf55>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    self.lstm = nn.LSTM(32, 64, batch_first=True, bidirectional=True;likujyhgfdsjkl;)\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dict_size=28):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dict_size, 64)\n",
    "        self.lstm = nn.LSTM(32, 64, batch_first=True, bidirectional=True;likujyhgfdsjkl;)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_input_sequences):\n",
    "        embedded = self.embedding(encoder_input_sequences)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return output[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dict_size=28):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dict_size, 64)\n",
    "        self.lstm = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.linear = nn.Linear(64, output_dict_size)\n",
    "\n",
    "    def forward(self, encoder_output, decoder_input_sequences):\n",
    "        encoder_output = encoder_output.unsqueeze(0)\n",
    "        embedded = self.embedding(decoder_input_sequences)\n",
    "        output, _ = self.lstm(embedded, [encoder_output, encoder_output])\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, encoder_input_sequences, decoder_input_sequences):\n",
    "        encoder_output = self.encoder(encoder_input_sequences)\n",
    "        decoder_output = self.decoder(encoder_output, decoder_input_sequences)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_CODE_START = 27\n",
    "INPUT_LENGTH = 33\n",
    "OUTPUT_LENGTH = 33\n",
    "\n",
    "gen_files()\n",
    "input_data, output_data = gen_examples()\n",
    "\n",
    "chars = set(' '.join(input_data)) | set(' '.join(output_data))\n",
    "int_to_char = dict(enumerate(chars, 1))\n",
    "int_to_char[28] = 'START'\n",
    "\n",
    "data_input = np.array([transform(d) for d in encode_list(input_data, chars, int_to_char)])\n",
    "data_output = np.array([transform(d) for d in encode_list(output_data, chars, int_to_char)])\n",
    "\n",
    "perm = np.random.permutation(data_input.shape[0])\n",
    "data_input = data_input[perm]\n",
    "data_output = data_output[perm]\n",
    "\n",
    "data_size = len(input_data)\n",
    "encoded_training_input = data_input\n",
    "encoded_training_output = data_output\n",
    "\n",
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_output = encoded_training_output\n",
    "\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = CHAR_CODE_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, batch_size=64, criterion=nn.CrossEntropyLoss(), encoder_input=training_encoder_input,decoder_input=training_decoder_input, decoder_output=training_decoder_output):\n",
    "    permutation = np.random.permutation(encoder_input.shape[0])\n",
    "    encoder_input = encoder_input[permutation]\n",
    "    decoder_input = decoder_input[permutation]\n",
    "    decoder_output = decoder_output[permutation]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    iteration_count = 0\n",
    "    \n",
    "    for begin_index in range(0, int(len(encoder_input)*0.1), batch_size):    \n",
    "        end_index = begin_index + batch_size\n",
    "        iteration_count += 1\n",
    "        \n",
    "        e_in = torch.tensor(encoder_input[begin_index:end_index]).to(torch.int64)\n",
    "        d_in = torch.tensor(decoder_input[begin_index:end_index]).to(torch.int64)\n",
    "        d_out = torch.tensor(decoder_output[begin_index:end_index]).to(torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(e_in, d_in)\n",
    "        target = d_out.view(-1)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss = epoch_loss + loss.item()\n",
    "            \n",
    "    return epoch_loss / iteration_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, n_epoch=30):\n",
    "    for i in range(1, n_epoch + 1):\n",
    "        loss = train_epoch(model, optimizer)\n",
    "        print('epoch ', i, ', loss: ', loss)\n",
    "\n",
    "def generate_output(input_sequence):\n",
    "    decoder_input = np.zeros(shape=(len(input_sequence), OUTPUT_LENGTH), dtype='int')\n",
    "    decoder_input[:,0] =  CHAR_CODE_START\n",
    "    \n",
    "    encoder_input = torch.tensor(input_sequence).to(torch.int64)\n",
    "    decoder_input = torch.tensor(decoder_input).to(torch.int64)\n",
    "    \n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        model.cpu()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        output = output.argmax(dim=2)\n",
    "        decoder_input[:,i] = output[:,i-1]\n",
    "        \n",
    "    return decoder_input[:,1:].detach().numpy()\n",
    "\n",
    "def decode_single(data, chars, int_to_char):\n",
    "    de_data = ''\n",
    "    data = data[0]\n",
    "    for ch in data:\n",
    "        if ch == 0:\n",
    "            break\n",
    "        if ch == 28:\n",
    "            continue\n",
    "        de_data += int_to_char[ch]\n",
    "    return de_data\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    translated = []\n",
    "    for word in words:\n",
    "        inp = [transform(encode_single(word, chars, int_to_char))]\n",
    "        out = generate_output(inp)\n",
    "        translated.append(decode_single(out, chars, int_to_char))\n",
    "    return ' '.join(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 , loss:  0.7918949072917532\n",
      "epoch  2 , loss:  0.7013959657905424\n",
      "epoch  3 , loss:  0.6853027545753061\n",
      "epoch  4 , loss:  0.6750695450161651\n",
      "epoch  5 , loss:  0.6700472347674521\n",
      "epoch  6 , loss:  0.6655973805817815\n",
      "epoch  7 , loss:  0.6467723959117526\n",
      "epoch  8 , loss:  0.5833438515491376\n",
      "epoch  9 , loss:  0.5112464204129981\n",
      "epoch  10 , loss:  0.44520724730120614\n",
      "epoch  11 , loss:  0.3855056169568977\n",
      "epoch  12 , loss:  0.34311881936249194\n",
      "epoch  13 , loss:  0.3117845743121606\n",
      "epoch  14 , loss:  0.2855542089135228\n",
      "epoch  15 , loss:  0.26448184335953223\n",
      "epoch  16 , loss:  0.24506816001721693\n",
      "epoch  17 , loss:  0.2306771616856715\n",
      "epoch  18 , loss:  0.2129188483317922\n",
      "epoch  19 , loss:  0.19964998507018736\n",
      "epoch  20 , loss:  0.1966515133978654\n",
      "epoch  21 , loss:  0.186656533632567\n",
      "epoch  22 , loss:  0.1740190249655707\n",
      "epoch  23 , loss:  0.16503155371898875\n",
      "epoch  24 , loss:  0.157720651093237\n",
      "epoch  25 , loss:  0.1510531153948575\n",
      "epoch  26 , loss:  0.14551580659508362\n",
      "epoch  27 , loss:  0.13975583286123935\n",
      "epoch  28 , loss:  0.1345452185623584\n",
      "epoch  29 , loss:  0.13075249785133328\n",
      "epoch  30 , loss:  0.12536234750441927\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train_model(model, optimizer, n_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uckfay onaldday umpsay'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(\"fuck donald trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
