{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["import numpy as np\n","from gensim.corpora.dictionary import Dictionary\n","from gensim.models.ldamodel import LdaModel as ldamodel\n","from gensim.models.ldamodel import LdaState as ldastate"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Question 1"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def lda(vocabulary, beta, alpha, xi):\n","    '''\n","    Args:\n","        vocabulary - list (of length V ) of strings\n","        beta - topic-word matrix, numpy array of size (k, V )\n","        alpha - topic distribution parameter vector, of length k\n","        xi - Poisson parameter (scalar) for document size distribution\n","    Returns:\n","        w - list of words (strings) in a document\n","    '''\n","    n = np.random.poisson(xi)\n","    theta = np.random.dirichlet(alpha, xi)\n","    doc = []\n","    for i in theta:\n","        t = beta[int(np.nonzero(np.random.multinomial(1, i))[0])]\n","        w = vocabulary[int(np.nonzero(np.random.multinomial(1, t))[0])]\n","        doc.append(w)\n","    return doc"],"execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Question 2"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def init_corpus(vocabulary, beta, alpha, xi, num_docs):\n","    corpus = []\n","    for i in range(0, num_docs):\n","        corpus.append(lda(vocabulary, beta, alpha, xi))\n","    return corpus"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def train_LDA(texts):\n","    common_dictionary = Dictionary(texts)\n","    common_corpus = [common_dictionary.doc2bow(text) for text in texts]\n","    lda = ldamodel(common_corpus, alpha='auto', eta='auto')\n","    return lda"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def model_params(lda, xi, num_topics):\n","    alpha= lda.alpha\n","    eta = lda.eta\n","    theta = np.random.dirichlet(lda.alpha, xi)\n","    beta = np.random.dirichlet(lda.eta, num_topics)\n","    return {'alpha': alpha, 'theta': theta, 'eta': eta, 'beta': beta}"],"execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["vocabulary = ['bass', 'pike', 'deep', 'tuba', 'horn', 'catapult']\n","beta = np.array([\n","    [0.4, 0.4, 0.2, 0.0, 0.0, 0.0],\n","    [0.0, 0.3, 0.1, 0.0, 0.3, 0.3],\n","    [0.3, 0.0, 0.2, 0.3, 0.2, 0.0]\n","])\n","alpha = np.array([1, 3, 8])\n","xi = 50\n","num_docs = 50\n","gen = lda(vocabulary, beta, alpha, xi)\n","corpus = init_corpus(vocabulary, beta, alpha, xi, num_docs)\n","model = train_LDA(corpus)\n","infer = model_params(model, xi, len(beta))\n","print(infer)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"{'alpha': array([0.00954627, 0.00954627, 0.01057715, 0.0105691 , 0.00996073,\n       0.00954627, 0.00954627, 0.00954627, 0.00954627, 0.00954627,\n       0.00954627, 0.01066995, 0.01141566, 0.01057802, 0.00954627,\n       0.00995284, 0.00954627, 0.00995826, 0.00954627, 0.01016877,\n       0.00954627, 0.00975517, 0.00974815, 0.01079038, 0.00954627,\n       0.00975471, 0.00954627, 0.00954627, 0.01037403, 0.00975229,\n       0.00954627, 0.00954627, 0.00954627, 0.01036659, 0.00954627,\n       0.00954627, 0.01058628, 0.00954627, 0.01079707, 0.00954627,\n       0.00954627, 0.01037332, 0.01058258, 0.01016742, 0.00975493,\n       0.00954627, 0.00954627, 0.00954627, 0.00954627, 0.00975155,\n       0.00958078, 0.00954627, 0.00954627, 0.00954627, 0.00954627,\n       0.00954627, 0.00954627, 0.00975104, 0.00954627, 0.00954627,\n       0.00954627, 0.00954627, 0.00954627, 0.00954627, 0.00954627,\n       0.00954627, 0.00954627, 0.00954627, 0.01078735, 0.01058296,\n       0.00954627, 0.00954627, 0.00954627, 0.00954627, 0.00954627,\n       0.00954627, 0.00954627, 0.00954627, 0.00954627, 0.00954627,\n       0.00954627, 0.00954627, 0.00954627, 0.00975127, 0.00995687,\n       0.00954627, 0.00954627, 0.00975014, 0.00975226, 0.00954627,\n       0.00954627, 0.00975517, 0.00975257, 0.00995206, 0.00954627,\n       0.00954627, 0.00975367, 0.00975481, 0.00954627, 0.00973579],\n      dtype=float32), 'theta': array([[2.46671475e-051, 3.01631367e-014, 9.05149743e-030, ...,\n        3.30621271e-007, 3.11164234e-022, 2.33715755e-021],\n       [2.23910899e-007, 9.91315569e-113, 3.67550732e-007, ...,\n        7.61472661e-003, 1.03449213e-044, 5.48650386e-011],\n       [3.49243843e-001, 1.90223691e-085, 3.03996992e-034, ...,\n        3.55234077e-059, 5.28505356e-018, 1.81913390e-115],\n       ...,\n       [4.82085464e-061, 6.63355568e-007, 1.87407390e-014, ...,\n        7.73244485e-016, 1.58472728e-004, 1.03528068e-021],\n       [1.63622147e-020, 1.36407559e-018, 1.25056798e-004, ...,\n        1.62538978e-124, 1.25103598e-011, 1.54780881e-013],\n       [1.03112088e-029, 2.21558426e-001, 2.03225721e-004, ...,\n        3.04308336e-025, 5.77609290e-004, 1.90679864e-015]]), 'eta': array([0.01343542, 0.01338735, 0.01342738, 0.01343271, 0.01341111,\n       0.01343406], dtype=float32), 'beta': array([[1.00000000e+000, 4.15946417e-013, 1.48620798e-028,\n        2.09957716e-046, 8.92195664e-023, 3.93097236e-118],\n       [2.12009764e-025, 1.57487037e-018, 4.53646080e-007,\n        3.23954301e-017, 2.45821905e-002, 9.75417356e-001],\n       [1.87706588e-013, 5.28195462e-015, 3.96464349e-042,\n        3.23727926e-004, 9.99676272e-001, 1.26032411e-121]])}\n"}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}